{
    "auto_start_runners": true,
    "api": {
        "host": "0.0.0.0",
        "port": 8080,
        "health_endpoint": "/health"
    },
    "retry_config": {
        "max_retries": 3,
        "base_delay_seconds": 2,
        "max_delay_seconds": 30,
        "retry_on_model_loading": true
    },
    "runner1": {
        "type": "llama-server",
        "path": "/usr/local/bin/llama-server",
        "host": "127.0.0.1",
        "port": 8085,
        "inherit_env": true,
        "env": {},
        "auto_unload_timeout_seconds": 300,
        "extra_args": ["--verbose"]
    },
    "runner2": {
        "type": "llama-server",
        "path": "/usr/local/bin/llama-server",
        "host": "127.0.0.1",
        "port": 8086,
        "inherit_env": true,
        "env": {},
        "auto_unload_timeout_seconds": 0,
        "extra_args": []
    },
    "models": [
        {
            "runner": "runner1",
            "model": "/app/models/model1.gguf",
            "model_alias": "model1",
            "n_ctx": 4096,
            "n_batch": 512,
            "n_threads": 4,
            "offload_kqv": true,
            "flash_attn": "on",
            "use_mlock": true,
            "main_gpu": 0,
            "tensor_split": [1.0, 0.0],
            "n_gpu_layers": 99,
            "jinja": true,
            "args": "--device Vulkan0"
        },
        {
            "runner": "runner2",
            "model": "/app/models/model2.gguf",
            "model_alias": "model2",
            "n_ctx": 16384,
            "n_batch": 256,
            "n_threads": 8,
            "chat_template": "mistral-instruct",
            "offload_kqv": false,
            "flash_attn": "auto",
            "use_mlock": false,
            "main_gpu": 0,
            "tensor_split": [1.0, 0.0],
            "n_gpu_layers": 0,
            "jinja": true,
            "split_mode": "row",
            "cache-type-k": "q8_0",
            "cache-type-v": "q8_0",
            "args": ""
        }
    ]
}