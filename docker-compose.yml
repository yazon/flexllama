services:
  flexllama:
    image: flexllama:latest
    build:
      context: .
      dockerfile: Dockerfile
    restart: unless-stopped
    profiles: ["cpu"]
    ports:
      - "8090:8080"  # FlexLLama API and dashboard
      - "8085:8085"  # Runner 1
      - "8086:8086"  # Runner 2
      - "8087:8087"  # Runner 3 (optional)
      - "8088:8088"  # Runner 4 (optional)
      - "8089:8089"  # Runner 5 (optional)
    volumes:
      - ./docker/config.json:/app/config.json:ro  # Configuration file
      - ./models:/app/models:ro                   # Model files directory
    environment:
      - FLEXLLAMA_CONFIG=/app/config.json
      - FLEXLLAMA_HOST=0.0.0.0
      - FLEXLLAMA_PORT=8080
    networks:
      - flexllama-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  flexllama-gpu:
    image: flexllama-gpu:latest
    build:
      context: .
      dockerfile: Dockerfile.cuda
    restart: unless-stopped
    profiles: ["gpu"]
    ports:
      - "8090:8080"  # FlexLLama API and dashboard
      - "8085:8085"  # Runner 1
      - "8086:8086"  # Runner 2
      - "8087:8087"  # Runner 3 (optional)
      - "8088:8088"  # Runner 4 (optional)
      - "8089:8089"  # Runner 5 (optional)
    volumes:
      - ./docker/config.json:/app/config.json:ro
      - ./models:/app/models:ro
    environment:
      - FLEXLLAMA_CONFIG=/app/config.json
      - FLEXLLAMA_HOST=0.0.0.0
      - FLEXLLAMA_PORT=8080
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all # Or specify a number of GPUs, e.g., 1
    networks:
      - flexllama-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

networks:
  flexllama-network:
    driver: bridge

volumes:
  models:
    driver: local
  logs:
    driver: local