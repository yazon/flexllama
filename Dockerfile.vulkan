# Multi-stage build for FlexLLama with Vulkan support (AMD and Intel GPUs)
# Stage 1: Build llama.cpp with Vulkan support
FROM ubuntu:24.04 AS builder

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    git \
    wget \
    pkg-config \
    libssl-dev \
    libcurl4-openssl-dev \
    libopenblas-dev \
    libvulkan-dev \
    glslang-tools \
    glslc \
    && rm -rf /var/lib/apt/lists/*

# Clone and build llama.cpp with Vulkan support
WORKDIR /build

RUN git clone https://github.com/ggerganov/llama.cpp.git && \
    cd llama.cpp && \
    cmake -B build \
        -DGGML_VULKAN=ON \
        -DGGML_BLAS=ON \
        -DGGML_BLAS_VENDOR=OpenBLAS \
        -DBUILD_SHARED_LIBS=ON \
        -DCMAKE_BUILD_TYPE=Release \
        -DCMAKE_INSTALL_PREFIX=/opt/llama.cpp \
        -DLLAMA_BUILD_SERVER=ON && \
    cmake --build build --config Release -j$(nproc) && \
    cmake --install build && \
    mkdir -p /opt/llama.cpp/lib && \
    find build -name "*.so*" -type f -exec cp -a {} /opt/llama.cpp/lib/ \; && \
    find build/tools/mtmd -name "*.so*" -type f -exec cp -a {} /opt/llama.cpp/lib/ \; || true

# Stage 2: Runtime environment with AMD/Intel Vulkan support
FROM ubuntu:24.04

# Install runtime dependencies with Vulkan support for AMD and Intel
RUN apt-get update && apt-get install -y \
    python3 \
    python3-venv \
    python3-pip \
    curl \
    jq \
    libgomp1 \
    libopenblas0 \
    libvulkan1 \
    vulkan-tools \
    mesa-vulkan-drivers \
    libxext6 \
    libx11-6 \
    libxau6 \
    libxdmcp6 \
    libxcb1 \
    gosu \
    ca-certificates \
    gnupg \
    && rm -rf /var/lib/apt/lists/* \
    && ln -s /usr/bin/python3 /usr/bin/python

# Intel Vulkan support (IRIS driver)
RUN apt-get update && apt-get install -y \
    intel-media-va-driver-non-free \
    intel-level-zero-loader \
    level-zero-loader \
    2>/dev/null || true && \
    rm -rf /var/lib/apt/lists/*

# Set up library paths
ENV LD_LIBRARY_PATH=/usr/local/lib:/usr/lib/x86_64-linux-gnu
ENV PATH=/usr/local/bin:${PATH}

# AMD RADV driver (via Mesa) - enable ACO compiler for better performance
ENV RADV_PERFTEST=aco

# Intel Arc GPU support
ENV ONEAPI_DEVICE_SELECTOR=level_zero

# Copy everything from the built location
COPY --from=builder /opt/llama.cpp/ /usr/local/

# Ensure all libraries are properly linked
RUN ldconfig && \
    echo "=== Vulkan Drivers ===" && \
    vulkaninfo 2>&1 | grep -E "(GPU|deviceName|MESA|Intel)" || echo "Vulkan info check complete"

# Create a virtual environment
ENV VENV_PATH=/opt/venv
RUN python3 -m venv $VENV_PATH
ENV PATH="$VENV_PATH/bin:$PATH"

# Create non-root user
RUN useradd -r -m -s /bin/bash flexllama

# Set working directory
WORKDIR /app

# Copy Python dependencies and install them
COPY pyproject.toml ./
RUN pip install --no-cache-dir .

# Copy application code
COPY main.py ./
COPY backend/ ./backend/
COPY frontend/ ./frontend/
COPY static/ ./static/
COPY docker/ ./docker/

# Copy and setup entrypoint script
COPY docker/entrypoint.sh /app/entrypoint.sh
RUN chmod +x /app/entrypoint.sh

# Create necessary directories and set permissions
RUN mkdir -p /app/models /app/config && \
    chown -R flexllama:flexllama /app $VENV_PATH

# Create default config from template
COPY docker/config.qwen3.unified.json /app/config.json

# Note: Container will start as root (via docker-compose user: "0:0")
# The entrypoint script will configure GPU permissions and switch to flexllama user

# Expose ports
# 8090: FlexLLama API and dashboard
# 8095-8100: Default ports for llama-server runners
EXPOSE 8090 8095 8096 8097 8098 8099 8100

# Environment variables
ENV FLEXLLAMA_CONFIG=/app/config.json
ENV FLEXLLAMA_HOST=0.0.0.0
ENV FLEXLLAMA_PORT=8090
ENV PYTHONPATH=/app
ENV VK_LOADER_DEBUG=error

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8090/health || exit 1

# Set entrypoint
ENTRYPOINT ["/app/entrypoint.sh"]

# Default command (can be overridden)
CMD []
