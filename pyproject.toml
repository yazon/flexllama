[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "flexllama"
version = "0.1.2"
description = "A lightweight Python tool that easily runs multiple llama.cpp server instances with OpenAI v1 API compatibility"
readme = "README.md"
license = {text = "BSD-3-Clause"}
authors = [
    {name = "Wojciech Czaplejewicz", email = "czaplejewicz@gmail.com"}
]
keywords = [
    "flexllama",
    "llama",
    "llama.cpp",
    "openai",
    "runner",
    "api",
    "llm",
    "language-model",
    "multi-gpu",
    "embedding",
    "reranking",
    "inference",
    "server"
]
requires-python = ">=3.12"
dependencies = [
    "aiohttp>=3.8.0",
    "psutil>=7.0.0",
]

[project.urls]
Homepage = "https://github.com/yazon/flexllama"

[project.scripts]
flexllama = "main:main_entry"

[tool.setuptools]
py-modules = ["main"]
include-package-data = true

[tool.setuptools.packages.find]
include = ["backend*", "frontend*"]
exclude = ["tests*", "logs*", "__pycache__*"]

[tool.setuptools.package-data]
backend = ["config_example.json"]
frontend = ["*"]
